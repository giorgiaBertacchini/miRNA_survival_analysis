{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda669cc",
   "metadata": {},
   "source": [
    "# DATA NORMALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d88d76",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b436641",
   "metadata": {},
   "source": [
    "Various factors affect transcript quantification in RNA-seq data, such as sequencing depth, transcript length, and sample-to-sample and batch-to-batch variability. Normalization methods exist to minimize these variables and ensure reliable transcriptomic data. Sequencing technologies introduce technical variability. Therefore, raw transcriptomic data must be adjusted to account for these technical factors. \n",
    "\n",
    "It is essential to choose the correct RNA-seq normalization method for the dataset and there are three main RNA-seq normalization stages to consider:\n",
    " 1. **Within sample**: Within sample normalization is required to compare the expression of genes within an individual sample. It can adjust data for two primary technical variables: transcript length and sequencing depth. Longer genes often have more mapped reads than shorter genes at the same expression level. Therefore, their expression level can only be accurately compared within a sample after normalization. Furthermore, the number of sequencing reads per sample may vary. This can also be corrected by within sample normalization. Within sample normalization is not sufficient to compare gene expression between samples. For this, between sample RNA-seq normalization methods are required. Within sample normalization most common techniques are:\n",
    "    - **CPM**: Counts per million (CPM) mapped reads are the number of raw reads mapped to a transcript, scaled by the number of sequencing reads in your sample, multiplied by a million. It normalizes RNA-seq data for sequencing depth but not gene length. Therefore, although it is a within sample normalization approach, CPM normalization is unsuitable for within sample comparisons of gene expression. Between sample comparisons can be made when CPM is used alongside ‘within a dataset’ normalization methods.\n",
    "    - **FPKM/RPKM**: FPKM (fragments per kilobase of transcript per million fragments mapped) for paired-end data and RPKM (reads per kilobase of transcript per million reads mapped) for single-end data correct for variations in library size and gene length. One issue with FPKM/RPKM units is that the expression of a gene in one sample will appear different from its expression in another sample, even when its true expression level is the same. This is because it depends on the relative abundance of a transcript among a population of sequenced transcripts. FPKM/RPKM units best compare gene expression within a single sample\n",
    "    - **TPM**: Transcripts per million (TPM) represents the relative number of transcripts you would detect for a gene if you had sequenced one million full-length transcripts. It is calculated by dividing the number of reads mapped to a transcript by the transcript length. This value is then divided by the sum of mapped reads to all transcripts after normalization for transcript length. It is then multiplied by one million to allow easier further analyses. It normalizes RNA-seq data for sequencing depth and transcript length. TPM and FPKM/RPKM are closely related, however, in contrast to FPKM/RPKM, there is limited variation in values between samples as the sum of all TPMs in each sample is the same. TPM can be used for within sample comparisons but requires ‘within a dataset’ normalization for between sample comparisons.\n",
    "\n",
    " 2. **Within a dataset (between samples)**: Samples within a dataset can be simultaneously normalized as a complete set to adjust for different technical variations such as sequencing depth. RNA-seq is a relative, not an absolute, measure of transcript abundance. This means that the transcript population as a whole affects relative levels of transcripts. This creates biases for gene expression analyses, and these are minimized by between sample RNA-seq normalization methods. Within a dataset normalization mosto common techniques are:\n",
    "    - **Quantile**: The quantile method aims to make the distribution of gene expression levels the same for each sample in a dataset. It assumes that the global differences in distributions between samples are all due to technical variation. Any remaining differences are likely actual biological effects. For each sample, genes are ranked based on their expression level. An average value is calculated across all samples for genes of the same rank. This average value then replaces the original value of all genes in that rank. These genes are then placed in their original order.\n",
    "    - **TMM**: TMM (trimmed mean of M-values) also assumes that most genes are not differentially expressed between samples. If many genes are uniquely or highly expressed in one experimental condition, it will affect the accurate quantification of the remaining genes. To adjust for this possibility, TMM calculates scaling factors to adjust library sizes for the normalization of samples within a dataset. To do this, one sample is chosen as a reference sample. The fold changes and absolute expression levels of other samples within the dataset are then calculated relative to the reference sample. Next, the genes in the data set are ‘trimmed’ to remove differentially expressed genes using these two values. The trimmed mean of the fold changes is then found for each sample. Finally, read counts are scaled by this trimmed mean and the total count of their sample.\n",
    "\n",
    " 3. **Across datasets**: Researchers often integrate RNA-seq data from multiple independent studies. These datasets are usually sequenced at different times, with varying methods across multiple facilities, and contain other experimental variables. This results in a batch effect. The batch effect is often responsible for the greatest source of differential expression when data is combined. It can mask any true biological differences and lead to incorrect conclusions. RNA-seq normalization across datasets can correct for known variables across batches, such as the sequencing center and date of sequencing, as well as unknown variables.\n",
    "\n",
    "source : https://bigomics.ch/blog/why-how-normalize-rna-seq-data/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b5bb5",
   "metadata": {},
   "source": [
    "**PERSONAL CONSIDERATIONS**\n",
    "\n",
    "Since we want to have a model capable of inferring the survivability of a patient given a feature vector, we thought that the model would need, to behave at best, data normalized across the dataset allowing intra-dataset comparisons in order to learn when a feature vector corresponds to higher or lower survivability. Given this preposition we first normalize miRNA read values with the TMM method to allow genetic intra-dataset comparisons, and then normalize all values of the feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e1516",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b19a22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:22:59.257776Z",
     "start_time": "2025-10-23T16:22:58.159659Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f47107ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:22:59.288372Z",
     "start_time": "2025-10-23T16:22:59.262349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Universita\\2 anno magistrale\\Progetto BioInf\\miRNA_to_age\n"
     ]
    }
   ],
   "source": [
    "base = os.path.basename(os.getcwd())\n",
    "list = os.getcwd().split(os.sep) \n",
    "list.pop(list.index(base))\n",
    "ROOT = '\\\\'.join(list)\n",
    "print(ROOT)\n",
    "DATA_PATH = os.path.join(ROOT, 'datasets\\\\preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9377bf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T16:22:59.769799Z",
     "start_time": "2025-10-23T16:22:59.326271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['days_to_death', 'pathologic_stage',\n",
      "       'age_at_initial_pathologic_diagnosis', 'days_to_last_followup', 'Death',\n",
      "       'case_id', 'miRNA_ID', 'read_count', 'reads_per_million_miRNA_mapped'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv(os.path.join(DATA_PATH, 'clinical_miRNA(RC_RPM).csv'))\n",
    "print(raw_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# print(data.shape)\n",
    "\n",
    "# tmp = data.drop(columns=['radiation_therapy', 'case_id'])\n",
    "# print(tmp.shape)\n",
    "# tmp = tmp.dropna(subset=['pathologic_stage'])\n",
    "# print(tmp.shape)\n",
    "\n",
    "# tmp.to_csv(os.path.join(DATA_PATH, 'clinical_miRNA.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772fa321",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3a0cd",
   "metadata": {},
   "source": [
    "Removed useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "519389f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(767, 9)\n",
      "(767, 8)\n"
     ]
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "raw_data.drop(columns=['case_id'], inplace=True)\n",
    "print(raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e6aa270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_array(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip(\"[]\")\n",
    "        return np.array(eval(x, {'np':np}))\n",
    "    \n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a9343",
   "metadata": {},
   "source": [
    "One-hot encoding of pathological stage and deletion of columns with less than 20 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8da3b5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(767, 8)\n",
      "Passing from one column to 12\n",
      "(767, 12)\n",
      "(767, 6)\n",
      "(767, 13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print(raw_data.shape)\n",
    "\n",
    "pathologic_stage = raw_data[['pathologic_stage']]\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoded_stages = encoder.fit_transform(pathologic_stage).toarray()\n",
    "encoded_columns = encoder.get_feature_names_out(['pathologic_stage'])\n",
    "print(f\"Passing from one column to {len(encoded_columns)}\")\n",
    "encoded_df = pd.DataFrame(encoded_stages, columns=encoded_columns, index=pathologic_stage.index)\n",
    "\n",
    "print(encoded_df.shape)\n",
    "index = encoded_df.sum().index\n",
    "encoded_df.drop(columns=[i for i in index if encoded_df.sum()[i]<20], inplace=True)\n",
    "print(encoded_df.shape)\n",
    "\n",
    "raw_data = pd.concat([raw_data, encoded_df], axis=1)\n",
    "raw_data = raw_data.drop(columns=['pathologic_stage'])\n",
    "print(raw_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b72ec",
   "metadata": {},
   "source": [
    "Deleting age outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2213fd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(767, 13)\n",
      "(746, 13)\n"
     ]
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "\n",
    "ages_distrib = raw_data['age_at_initial_pathologic_diagnosis'].value_counts()\n",
    "ages_to_del = ages_distrib[ages_distrib < 5].index.tolist()\n",
    "mask = raw_data['age_at_initial_pathologic_diagnosis'].isin(ages_to_del)\n",
    "indexes = raw_data[mask].index.to_list()\n",
    "raw_data.drop(indexes, inplace=True)\n",
    "print(raw_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef14c31",
   "metadata": {},
   "source": [
    "deleting rows that are not dead people or patients with high last_days_to_followup (25-th percentile of dead patients days_to_death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29d6cdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(746, 13)\n",
      "\n",
      "Number of dead patients: 68\n",
      "count      68.000000\n",
      "mean     1521.117647\n",
      "std      1083.264132\n",
      "min         1.000000\n",
      "25%       602.250000\n",
      "50%      1269.500000\n",
      "75%      2384.000000\n",
      "max      4456.000000\n",
      "Name: days_to_death, dtype: float64\n",
      "\n",
      "Number of alive patients with days_to_last_followup > 602.25: 259\n",
      "(327, 13)\n"
     ]
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "\n",
    "dead = raw_data[raw_data['Death'] == 1]\n",
    "print(f\"\\nNumber of dead patients: {dead.shape[0]}\")\n",
    "describe = dead.describe()['days_to_death']\n",
    "print(describe)\n",
    "alive = raw_data[(raw_data['days_to_last_followup']>describe['25%']) & (raw_data['Death']==0)]\n",
    "print(f\"\\nNumber of alive patients with days_to_last_followup > {describe['25%']}: {alive.shape[0]}\")\n",
    "\n",
    "raw_data = pd.concat([dead, alive], axis=0)\n",
    "print(raw_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec543277",
   "metadata": {},
   "source": [
    "extraction of miRNA_reads and adjustment into dataframe with each column being a gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2fa38438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:40:31.312716Z",
     "start_time": "2025-10-23T18:40:31.262718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used existing file\n",
      "           hsa-let-7a-1  hsa-let-7a-2  hsa-let-7a-3    hsa-let-7b  \\\n",
      "Sample_4    4818.534597   4785.049036   4915.228857   9827.447776   \n",
      "Sample_25   4282.757713   4320.487207   4417.571638  12548.737764   \n",
      "Sample_37  14109.535620  14138.877137  14181.022226  29818.450693   \n",
      "Sample_74   7677.699119   7596.189759   7757.599742  21011.718847   \n",
      "Sample_76   7889.027302   7703.268940   7950.132027   6782.624423   \n",
      "\n",
      "            hsa-let-7c  hsa-let-7d   hsa-let-7e  hsa-let-7f-1  hsa-let-7f-2  \\\n",
      "Sample_4   1689.327738  667.830008  4157.853192   1991.450271   2011.014868   \n",
      "Sample_25  1737.857314  884.802652  1044.462829   1526.664170   1525.743938   \n",
      "Sample_37  2817.319178  378.238837  1458.540169   7162.531202   7317.774505   \n",
      "Sample_74  2165.092383  841.905893  1783.553502   3637.355204   3632.797114   \n",
      "Sample_76  1008.635320  215.088630   615.935623    808.211823    791.102500   \n",
      "\n",
      "           hsa-let-7g  ...  hsa-mir-941-5  hsa-mir-942  hsa-mir-943  \\\n",
      "Sample_4   780.702685  ...            0.0     4.138665          0.0   \n",
      "Sample_25  510.268404  ...            0.0     4.141042          0.0   \n",
      "Sample_37  702.062495  ...            0.0     2.133929          0.0   \n",
      "Sample_74  424.438544  ...            0.0     2.949352          0.0   \n",
      "Sample_76  316.115108  ...            0.0    17.109323          0.0   \n",
      "\n",
      "           hsa-mir-944  hsa-mir-95  hsa-mir-9500  hsa-mir-96  hsa-mir-98  \\\n",
      "Sample_4      0.000000    4.514907           0.0   66.971122   88.040688   \n",
      "Sample_25     1.840463    0.460116           0.0   71.317946   83.280957   \n",
      "Sample_37     1.600446    0.533482           0.0   27.207589   88.024553   \n",
      "Sample_74     0.536246    5.898704           0.0   65.421987   75.074411   \n",
      "Sample_76     0.000000    4.073648           0.0   61.919454   33.403916   \n",
      "\n",
      "           hsa-mir-99a    hsa-mir-99b  \n",
      "Sample_4    480.837606  164538.640456  \n",
      "Sample_25   689.713560   47595.756996  \n",
      "Sample_37   917.055797   40489.693925  \n",
      "Sample_74   652.343005   89119.224331  \n",
      "Sample_76   176.796336   38759.948868  \n",
      "\n",
      "[5 rows x 1881 columns]\n",
      "(327, 1881)\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(DATA_PATH, 'genes_reads(RPM).csv')):\n",
    "    print('Used existing file')\n",
    "    genes_reads = pd.read_csv(os.path.join(DATA_PATH, 'genes_reads(RPM).csv'), index_col=0)\n",
    "    print(genes_reads.head())\n",
    "\n",
    "else:\n",
    "    print('Creating new file')\n",
    "    n_cols = len(raw_data[\"reads_per_million_miRNA_mapped\"][0].strip(\"[]\").split(\",\"))\n",
    "    reads = raw_data[\"reads_per_million_miRNA_mapped\"].apply(parse_array)\n",
    "\n",
    "    genes_reads = pd.DataFrame(\n",
    "        np.stack(reads.values),  # Converte la Series di vettori in un array 2D\n",
    "        index=[f'Sample_{i}' for i in raw_data.index],\n",
    "        columns = np.array(eval(raw_data['miRNA_ID'][0].strip('[]'), {'np':str}))\n",
    "    )\n",
    "\n",
    "    genes_reads.to_csv(os.path.join(DATA_PATH, 'genes_reads(RPM).csv'))\n",
    "    print(genes_reads.head())\n",
    "\n",
    "print(genes_reads.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec42e51",
   "metadata": {},
   "source": [
    "### Quantile and Log2 normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0b71cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           hsa-let-7a-1  hsa-let-7a-2  hsa-let-7a-3    hsa-let-7b   hsa-let-7c\n",
      "Sample_4    4818.534597   4785.049036   4915.228857   9827.447776  1689.327738\n",
      "Sample_25   4282.757713   4320.487207   4417.571638  12548.737764  1737.857314\n",
      "Sample_37  14109.535620  14138.877137  14181.022226  29818.450693  2817.319178\n",
      "Sample_74   7677.699119   7596.189759   7757.599742  21011.718847  2165.092383\n",
      "Sample_76   7889.027302   7703.268940   7950.132027   6782.624423  1008.635320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matteo Bulgarelli\\AppData\\Local\\Temp\\ipykernel_10668\\2233587725.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  reads_logged = genes_reads.applymap(lambda x: np.log2(float(x) + 1e-3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           hsa-let-7a-1  hsa-let-7a-2  hsa-let-7a-3  hsa-let-7b  hsa-let-7c\n",
      "Sample_4      12.234379     12.224318     12.263043   13.262601   10.722234\n",
      "Sample_25     12.064325     12.076979     12.109038   13.615255   10.763095\n",
      "Sample_37     13.784383     13.787380     13.791674   14.863918   11.460108\n",
      "Sample_74     12.906458     12.891060     12.921395   14.358907   11.080214\n",
      "Sample_76     12.945632     12.911255     12.956763   12.727628    9.978190\n",
      "           hsa-let-7a-1  hsa-let-7a-2  hsa-let-7a-3  hsa-let-7b  hsa-let-7c\n",
      "Sample_4      -7.651968     -7.651968     -7.651968   -7.687666   -6.768681\n",
      "Sample_25     -7.887958     -7.887958     -7.857447   -7.371142   -6.740548\n",
      "Sample_37     -4.561636     -4.561636     -4.583317   -5.892996   -5.720627\n",
      "Sample_74     -6.864840     -6.875526     -6.850876   -6.753876   -6.340340\n",
      "Sample_76     -6.768681     -6.850876     -6.768681   -7.967716   -7.325029\n"
     ]
    }
   ],
   "source": [
    "import qnorm\n",
    "\n",
    "print(genes_reads.iloc[:5, :5])\n",
    "\n",
    "# Log2 transform necessary for 0 values before quantile normalization\n",
    "reads_logged = genes_reads.applymap(lambda x: np.log2(float(x) + 1e-3))\n",
    "print(reads_logged.iloc[:5, :5])\n",
    "\n",
    "# Quantile normalization\n",
    "quant_norm = qnorm.quantile_normalize(reads_logged)\n",
    "print(quant_norm.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a8070",
   "metadata": {},
   "source": [
    "Delete reads columns with variance under the 50th percentile of the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0eb8a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(327, 1881)\n",
      "Columns to drop: 940\n",
      "(327, 941)\n"
     ]
    }
   ],
   "source": [
    "print(reads_logged.shape)\n",
    "zero_var_cols = reads_logged.var()[reads_logged.var()<reads_logged.var().describe().loc['50%']].index\n",
    "print(f\"Columns to drop: {len(zero_var_cols)}\")\n",
    "reads_logged.drop(columns=[i for i in reads_logged.columns if i in zero_var_cols], inplace=True)\n",
    "print(reads_logged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bda576b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(327, 1881)\n",
      "Columns to drop:939\n",
      "(327, 942)\n"
     ]
    }
   ],
   "source": [
    "print(quant_norm.shape)\n",
    "quant_norm_cols = quant_norm.var()[quant_norm.var()<quant_norm.var().describe().loc['50%']].index\n",
    "print(f\"Columns to drop:{len(quant_norm_cols)}\")\n",
    "quant_norm.drop(columns=[i for i in quant_norm.columns if i in quant_norm_cols], inplace=True)\n",
    "print(quant_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ca679",
   "metadata": {},
   "source": [
    "### Creation of normalized file with just log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d180a663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new dataset\n",
      "    days_to_death  age_at_initial_pathologic_diagnosis  days_to_last_followup  \\\n",
      "4          2763.0                                   46                 2763.0   \n",
      "25         4456.0                                   50                 4456.0   \n",
      "37         2520.0                                   55                 2520.0   \n",
      "74          538.0                                   79                  538.0   \n",
      "76         2551.0                                   47                 2551.0   \n",
      "\n",
      "    Death  pathologic_stage_Stage I  pathologic_stage_Stage IA  \\\n",
      "4       1                       1.0                        0.0   \n",
      "25      1                       0.0                        0.0   \n",
      "37      1                       1.0                        0.0   \n",
      "74      1                       0.0                        0.0   \n",
      "76      1                       0.0                        0.0   \n",
      "\n",
      "    pathologic_stage_Stage IIA  pathologic_stage_Stage IIB  \\\n",
      "4                          0.0                         0.0   \n",
      "25                         0.0                         1.0   \n",
      "37                         0.0                         0.0   \n",
      "74                         1.0                         0.0   \n",
      "76                         0.0                         0.0   \n",
      "\n",
      "    pathologic_stage_Stage IIIA  pathologic_stage_Stage IIIC  ...  \\\n",
      "4                           0.0                          0.0  ...   \n",
      "25                          0.0                          0.0  ...   \n",
      "37                          0.0                          0.0  ...   \n",
      "74                          0.0                          0.0  ...   \n",
      "76                          1.0                          0.0  ...   \n",
      "\n",
      "    hsa-mir-9-3  hsa-mir-934  hsa-mir-935  hsa-mir-937  hsa-mir-938  \\\n",
      "4      8.097534    -0.408349     3.049340     0.912428    -9.965784   \n",
      "25     8.649910     0.466076    -1.116798     2.339786    -9.965784   \n",
      "37     6.350917    -9.965784    -9.965784     1.094188    -9.965784   \n",
      "74     7.704600    -0.896345    -0.312278     1.271489    -9.965784   \n",
      "76    14.104539    -9.965784     2.289651     3.026499    -9.965784   \n",
      "\n",
      "    hsa-mir-939  hsa-mir-940  hsa-mir-943  hsa-mir-944  hsa-mir-95  \n",
      "4     -1.406438     3.233743    -9.965784    -9.965784    2.175016  \n",
      "25     0.880852     3.580629    -9.965784     0.880852   -1.116798  \n",
      "37    -0.903787     2.553189    -9.965784     0.679375   -0.903787  \n",
      "74     0.686825     3.055336    -9.965784    -0.896345    2.560643  \n",
      "76    -9.965784     2.874515    -9.965784    -9.965784    2.026675  \n",
      "\n",
      "[5 rows x 951 columns]\n",
      "(327, 951)\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(DATA_PATH, 'clinical_miRNA_normalized_log.csv')):\n",
    "    print(\"Loading existing dataset\")\n",
    "    dataset = pd.read_csv(os.path.join(DATA_PATH, 'clinical_miRNA_normalized_log.csv'))    \n",
    "\n",
    "else:\n",
    "    print(\"Creating new dataset\")\n",
    "    dataset = raw_data.copy()\n",
    "    dataset.drop(columns=['read_count', 'reads_per_million_miRNA_mapped', 'miRNA_ID'], inplace=True)\n",
    "    \n",
    "    reads_logged.index = dataset.index\n",
    "    dataset = pd.concat([dataset, reads_logged], axis=1)\n",
    "    dataset.to_csv(os.path.join(DATA_PATH, 'clinical_miRNA_normalized_log.csv'), index=False)\n",
    "\n",
    "print(dataset.head())\n",
    "print(dataset.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fbf73b",
   "metadata": {},
   "source": [
    "### Creation of normalized file with also quantile norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4413a977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new dataset\n",
      "    days_to_death  age_at_initial_pathologic_diagnosis  days_to_last_followup  \\\n",
      "4          2763.0                                   46                 2763.0   \n",
      "25         4456.0                                   50                 4456.0   \n",
      "37         2520.0                                   55                 2520.0   \n",
      "74          538.0                                   79                  538.0   \n",
      "76         2551.0                                   47                 2551.0   \n",
      "\n",
      "    Death  pathologic_stage_Stage I  pathologic_stage_Stage IA  \\\n",
      "4       1                       1.0                        0.0   \n",
      "25      1                       0.0                        0.0   \n",
      "37      1                       1.0                        0.0   \n",
      "74      1                       0.0                        0.0   \n",
      "76      1                       0.0                        0.0   \n",
      "\n",
      "    pathologic_stage_Stage IIA  pathologic_stage_Stage IIB  \\\n",
      "4                          0.0                         0.0   \n",
      "25                         0.0                         1.0   \n",
      "37                         0.0                         0.0   \n",
      "74                         1.0                         0.0   \n",
      "76                         0.0                         0.0   \n",
      "\n",
      "    pathologic_stage_Stage IIIA  pathologic_stage_Stage IIIC  ...  \\\n",
      "4                           0.0                          0.0  ...   \n",
      "25                          0.0                          0.0  ...   \n",
      "37                          0.0                          0.0  ...   \n",
      "74                          0.0                          0.0  ...   \n",
      "76                          1.0                          0.0  ...   \n",
      "\n",
      "    hsa-mir-939  hsa-mir-940  hsa-mir-942  hsa-mir-943  hsa-mir-944  \\\n",
      "4     -7.296851    -5.396933    -6.340340    -6.480528    -7.537047   \n",
      "25    -5.746550    -4.910072    -6.324768    -6.480528    -5.382199   \n",
      "37    -7.218936    -6.143993    -7.225741    -6.480528    -5.657541   \n",
      "74    -5.934445    -5.667496    -6.850876    -6.480528    -6.705291   \n",
      "76    -7.780318    -5.925272    -3.302045    -6.480528    -7.537047   \n",
      "\n",
      "    hsa-mir-95  hsa-mir-96  hsa-mir-98  hsa-mir-99a  hsa-mir-99b  \n",
      "4    -5.720627   -4.699533   -5.279473    -6.861303    -1.452108  \n",
      "25   -7.760968   -4.490614   -5.522994    -6.131357    -5.736363  \n",
      "37   -7.651968   -6.670862   -5.306056    -5.324673    -6.218226  \n",
      "74   -5.091822   -4.808623   -5.925272    -6.254979    -3.601961  \n",
      "76   -5.949759   -5.008744   -7.514728    -7.638173    -6.399121  \n",
      "\n",
      "[5 rows x 952 columns]\n",
      "(327, 952)\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(DATA_PATH, 'clinical_miRNA_normalized_quant.csv')):\n",
    "    print(\"Loading existing dataset\")\n",
    "    dataset = pd.read_csv(os.path.join(DATA_PATH, 'clinical_miRNA_normalized_quant.csv'))\n",
    "\n",
    "else:\n",
    "    print(\"Creating new dataset\")\n",
    "    dataset = raw_data.copy()\n",
    "    dataset.drop(columns=['read_count', 'reads_per_million_miRNA_mapped', 'miRNA_ID'], inplace=True)\n",
    "    \n",
    "    quant_norm.index = dataset.index\n",
    "    dataset = pd.concat([dataset, quant_norm], axis=1)\n",
    "    dataset.to_csv(os.path.join(DATA_PATH, 'clinical_miRNA_normalized_quant.csv'), index=False)\n",
    "\n",
    "print(dataset.head())\n",
    "print(dataset.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tirocinio-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
